\chapter{Ranking}

% ---------- @K Metrics ----------
\clearpage
\thispagestyle{rankingstyle}
\section{@K Metrics}
\subsection{@K Metrics}

% ---------- Mean Reciprocal Rank ----------
\clearpage
\thispagestyle{rankingstyle}
\section{Mean Reciprocal Rank}
\subsection{MRR}

The Mean Reciprocal Rank (MRR) is a metric that evaluates the quality of ranked results, typically in information retrieval
and recommendation systems. It measures how high the first relevant result appears in the ranking list. MRR is computed as
the average reciprocal rank across all queries in a dataset.

\begin{center}
    FORMULA GOES HERE
\end{center}

MRR goes from 0 to 1. Where 0 mean no relevant items exist in the ranked results for all queries and 1 is achieved when the
relevant item is always the first result for every query. In practice a low MRR suggests that users may need to scroll
significantly to find the relevant result, leading to poor user experience.

\textbf{When to use MRR?}

Use MRR when the primary goal is to return the most relevant result as close to the top of the list as possible, or
when evaluating the quality of systems where only the position of the first relevant result matters.

\coloredboxes{
    \item Intuitive metric.
    \item Aligns with user expectations in applications where relevance at the top matters most.
}
{
    \item MRR only considers the first relevant result, ignoring subsequent relevant results that might also be important.
    \item Systems returning shorter rankings can sometimes inflate MRR if the first relevant result appears earlier.
    \item MRR doesn't handle well cases where multiple relevant items exist with varying degrees of importance.

}

\clearpage

\thispagestyle{customstyle}

\textbf{Other related metrics}

MRR is a concise and effective metric for assessing ranked output but should be complemented by other metrics like
nDCG (Normalized Discounted Cumulative Gain) or Mean Average Precision when evaluating scenarios with multiple relevant
items or varying relevance grades.


% ---------- Mean Average Precision ----------
\clearpage
\thispagestyle{rankingstyle}
\section{Mean Average Precision}
\subsection{Mean Average Precision}

Mean Average Precision (MAP) is a metric used to evaluate the performance of ranking and recommendation systems. It measures
both the relevance of the recommended items and the order in which they are presented, rewarding systems that rank relevant
items higher. The MAP is calculated by averaging the Average Precision (AP) across all users or queries. 

\begin{center}
    FORMULA GOES HERE
\end{center}

MAP values range from 0 to 1, with 1 indicating a perfect ranking where all relevant items are placed at the top of the list.

\textbf{When to use MAP?}

MAP is particularly useful when evaluating search engines, recommendation systems, and other ranking-based models where
returning relevant items in the top ranks is critical. It is especially suited for situations where multiple queries exist,
and precision at various cutoffs matters more than recall.

\coloredboxes{
    \item Evaluates how well relevant items are ranked.
    \item Handles multiple queries. MAP averages precision over multiple search instances, providing a holistic evaluation
    of the model.
}
{
    \item Sensitive to exact ranking positions. A single misplaced relevant item can significantly impact MAP, making it less
    robust to minor ranking fluctuations.
    \item Requires known relevance labels. MAP relies on predefined relevance judgments, which may not always be
    available or objective.
    \item Complex interpretation.

}

% good resources for inpiration for visual: https://juneandrews.com/2014/12/15/mean-average-precision-isnt-so-nice/
% https://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html

% ---------- Hit Rate ----------
\clearpage
\thispagestyle{rankingstyle}
\section{Hit Rate}
\subsection{Hit Rate}

Hit Rate is a ranking metric used in recommender systems to measure how often a relevant item appears in the top-k
recommendations. It is a binary metric that checks whether at least one of the user's relevant items is present in the
recommended list. If at least one relevant item appears, it counts as a "hit"; otherwise, it does not.

\begin{center}
    FORMULA GOES HERE
\end{center}

Hit Rate is particularly useful in top-k recommendation scenarios where the primary goal is to ensure that users receive at
least one relevant item in their recommendation lists.

\textbf{When to use Hit Rate?}

Use Hit Rate when evaluating recommender systems that generate ranked lists of recommendations, such as e-commerce product
recommendations, streaming service content suggestions, or news article recommendations. It is best suited for use cases
where delivering at least one relevant recommendation is critical, rather than ranking all relevant items perfectly.

\coloredboxes{
    \item Simple to compute and interpret. Hit Rate provides an intuitive measure of whether the recommender system is
    surfacing at least one relevant item.
    \item Effective for top-k evaluation. This metric is particularly useful when users engage with only a few of
    the top-ranked recommendations.
    \item Works well for sparse data. Since it only requires identifying one correct prediction in the top-k list,
    it is less sensitive to sparsity compared to other ranking metrics.
}
{
    \item Hit Rate does not account for the position of the relevant item within the top-k list. An item in the first
    position has the same weight as an item in the last position.
    \item Hit Rate does not differentiate between multiple relevant hits. This means that if multiple relevant
    items appear in the top-k list, Hit Rate does not increase—it remains the same as long as at least one relevant
    item is found.
}

% ---------- Cumulative Gain ----------
\clearpage
\thispagestyle{rankingstyle}
\section{CG}
\subsection{Cumulative Gain}

Cumulative Gain (CG) measures the total relevance score of recommended items, by summing the relevance scores of all
items in the recommendation list. It provides a simple way to evaluate how much useful content is retrieved but
does not consider the position of the relevant items.

\begin{center}
    FORMULA GOES HERE
\end{center}

A higher cumulative gain reflects a greater concentration of relevant items in the recommendations,
while a lower cumulative gain indicates fewer relevant items in the list.

\textbf{When to use CG?}

CG is useful when you want a basic measure of total relevance in a recommendation list, without considering the ranking order.
It can be applied in scenarios where all retrieved relevant items are equally important, regardless of position.


\coloredboxes{
    \item CG provides a straightforward way to measure how the overall relevance score in the recommended list. 
    \item If the order of recommended items does not matter, CG is a reasonable metric.
}
{
    \item It does not penalize relevant items appearing lower in the ranking, which is
    unrealistic in most real-world applications.
    \item As with CG, DCG does not provide an upper bound.
    \item Human data labelers must provide relevance scores to compute the total accumulated relevance.
}


% ---------- Discounted Cumulative Gain ----------
\clearpage
\thispagestyle{rankingstyle}
\section{DCG}
\subsection{Discounted Cumulative Gain}

Discounted Cumulative Gain (DCG) improves upon CG by introducing a position-based discounting factor.
It accounts for the fact that relevant items appearing earlier in the list are more valuable than those appearing later.

\begin{center}
    FORMULA GOES HERE
\end{center}

This logarithmic discounting ensures that higher-ranked relevant items contribute more to the score.

\textbf{When to use DCG?}

Use DCG when you want to measure both the total relevance and the order of relevant items in a ranked list. It is particularly
useful for search engines, recommendation systems, and ranking models where placing relevant items at the top is crucial.

\coloredboxes{
    \item Accounts for ranking order. Relevant items appearing earlier in the list are weighted more heavily.
    \item More realistic than CG. Since users are more likely to engage with items at the top, DCG better reflects
    real-world relevance.
}
{
    \item It does not penalize relevant items appearing lower in the ranking, which is
    unrealistic in most real-world applications.
    \item Like CG, DCG does not provide an upper bound.
    \item Human data labelers must provide relevance scores to compute the total accumulated relevance.
}

% ---------- Normalized Discounted Cumulative Gain ----------
\clearpage
\thispagestyle{rankingstyle}
\section{nDCG}
\subsection{Normalized Discounted Cumulative Gain}

Normalized Discounted Cumulative Gain (nDCG) extends DCG by normalizing it relative to the Ideal Discounted Cumulative Gain
(IDCG)—the best possible ranking of the same set of items. This ensures that scores are comparable across different users
and queries.

\begin{center}
    FORMULA GOES HERE
\end{center}

IDCG is the DCG computed with an optimally ranked list. nDCG values range from 0 to 1, with 1 indicating a perfect ranking.

\textbf{When to use nDCG?}

Use nDCG when you need a standardized ranking metric that allows comparison across different recommendation tasks.
It is widely used in search engines, recommender systems, and ranking models where ranking quality is essential.

\coloredboxes{
    \item Scale-independent. Since it is normalized, NDCG values are comparable across different datasets and users.
    \item Captures both relevance and ranking order. It rewards placing highly relevant items early in the list while
    penalizing poorly ranked relevant items.
}
{
    \item It does not penalize relevant items appearing lower in the ranking, which is
    unrealistic in most real-world applications.
    \item Like CG, DCG does not provide an upper bound.
    \item Human data labelers must provide relevance scores to compute the total accumulated relevance.
    \item Not ideal for binary relevance. NDCG may not be the best fit in scenarios where relevance is strictly binary, as it
    is designed to handle graded relevance scores.
}

% great resource: https://aman.ai/recsys/metrics/#normalized-discounted-cumulative-gain-ndcg-1

% ---------- Behavioral Metrics (Novelty, Serendipity, Diversity/Intra-List Diversity, Coverage) ----------
\clearpage
\thispagestyle{rankingstyle}
\section{Behavioral Metrics}
\subsection{Behavioral Metrics}


