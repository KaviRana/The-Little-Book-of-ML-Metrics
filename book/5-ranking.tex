\chapter{Ranking}

% ---------- @K Metrics ----------
\clearpage
\thispagestyle{rankingstyle}
\section{@K Metrics}
\subsection{@K Metrics}

% ---------- Mean Reciprocal Rank ----------
\clearpage
\thispagestyle{rankingstyle}
\section{Mean Reciprocal Rank}
\subsection{MRR}

The Mean Reciprocal Rank (MRR) is a metric that evaluates the quality of ranked results, typically in information retrieval
and recommendation systems. It measures how high the first relevant result appears in the ranking list. MRR is computed as
the average reciprocal rank across all queries in a dataset.

\begin{center}
    FORMULA GOES HERE
\end{center}

MRR goes from 0 to 1. Where 0 mean no relevant items exist in the ranked results for all queries and 1 is achieved when the
relevant item is always the first result for every query. In practice a low MRR suggests that users may need to scroll
significantly to find the relevant result, leading to poor user experience.

\textbf{When to use MRR?}

Use MRR when the primary goal is to return the most relevant result as close to the top of the list as possible, or
when evaluating the quality of systems where only the position of the first relevant result matters.

\coloredboxes{
    \item Intuitive metric.
    \item Aligns with user expectations in applications where relevance at the top matters most.
}
{
    \item MRR only considers the first relevant result, ignoring subsequent relevant results that might also be important.
    \item Systems returning shorter rankings can sometimes inflate MRR if the first relevant result appears earlier.
    \item MRR doesn't handle well cases where multiple relevant items exist with varying degrees of importance.

}

\clearpage

\thispagestyle{customstyle}

\textbf{Other related metrics}

MRR is a concise and effective metric for assessing ranked output but should be complemented by other metrics like
nDCG (Normalized Discounted Cumulative Gain) or Mean Average Precision when evaluating scenarios with multiple relevant
items or varying relevance grades.


% ---------- Mean Average Precision ----------
\clearpage
\thispagestyle{rankingstyle}
\section{Mean Average Precision}
\subsection{Mean Average Precision}

Mean Average Precision (MAP) is a metric used to evaluate the performance of ranking and recommendation systems. It measures
both the relevance of the recommended items and the order in which they are presented, rewarding systems that rank relevant
items higher. The MAP is calculated by averaging the Average Precision (AP) across all users or queries. 

\begin{center}
    FORMULA GOES HERE
\end{center}

MAP values range from 0 to 1, with 1 indicating a perfect ranking where all relevant items are placed at the top of the list.

\textbf{When to use MAP?}

MAP is particularly useful when evaluating search engines, recommendation systems, and other ranking-based models where
returning relevant items in the top ranks is critical. It is especially suited for situations where multiple queries exist,
and precision at various cutoffs matters more than recall.

\coloredboxes{
    \item Evaluates how well relevant items are ranked.
    \item Handles multiple queries. MAP averages precision over multiple search instances, providing a holistic evaluation
    of the model.
}
{
    \item Sensitive to exact ranking positions. A single misplaced relevant item can significantly impact MAP, making it less
    robust to minor ranking fluctuations.
    \item Requires known relevance labels. MAP relies on predefined relevance judgments, which may not always be
    available or objective.
    \item Complex interpretation.

}

% good resources for inpiration for visual: https://juneandrews.com/2014/12/15/mean-average-precision-isnt-so-nice/
% https://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html

% ---------- Hit Rate ----------
\clearpage
\thispagestyle{rankingstyle}
\section{Hit Rate}
\subsection{Hit Rate}

% ---------- Normalized Discounted Cumulative Gain ----------
\clearpage
\thispagestyle{rankingstyle}
\section{Normalized Discounted Cumulative Gain}
\subsection{Normalized Discounted Cumulative Gain}

% ---------- Intra-List Diversity ----------
\clearpage
\thispagestyle{rankingstyle}
\section{Intra-List Diversity}
\subsection{Intra-List Diversity}

% ---------- Coverage ----------
\clearpage
\thispagestyle{rankingstyle}
\section{Coverage}
\subsection{Coverage}

% ---------- Behavioral Metrics (Novelty, Serendipity, Diversity) ----------
\clearpage
\thispagestyle{rankingstyle}
\section{Behavioral Metrics}
\subsection{Behavioral Metrics}


