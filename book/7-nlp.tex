\chapter{NLP}


% ---------- Bilingual Evaluation Understudy ----------
\clearpage
\thispagestyle{nlpstyle}
\section{BLEU}
\subsection{Bilingual Evaluation Understudy}
% references:
% https://aclanthology.org/P02-1040.pdf



BLEU is one of the earliest and most widely used metrics for evaluating machine translation and other
text generation tasks. It measures how similar a machine-generated text is to one or more human
reference translations by calculating the overlap of n-grams (contiguous sequences of words).

% equation
\begin{center}
    FORMULA GOES HERE
\end{center}

BLEU ranges from 0 to 1, where 1 indicates a perfect match with the reference.

\textbf{When to use BLEU?}

Use BLEU when evaluating machine translation, summarization, or text generation models where direct
word or phrase overlap with human-produced references is important.

\coloredboxes{
\item Correlates with human judgments at the corpus level. In its original study, BLEU showed strong
alignment with human evaluators.
\item Supports multiple references. Several human translations can be used, helping BLEU capture
natural variability.
\item To prevent models from producing unrealistically short outputs, BLEU introduces a
brevity penalty.
}
{
\item Insensitive to meaning. BLEU measures surface-level word overlap and does not account for
semantic equivalence.
\item Vulnerable to synonyms and paraphrasing. Valid alternative phrasings are not rewarded.
\item Sensitive to tokenization and normalization choices. BLEU scores can vary depending on how words
are segmented (e.g., handling of punctuation, casing, or subword units).
}

\clearpage

\thispagestyle{customstyle}

\orangebox{Did you know that...}
{BLEU was introduced in 2002 by Papineni et al. at IBM and was the first automatic metric to gain
wide acceptance in machine translation research.}


% ---------- Metric for Evaluation of Translation with Explicit ORdering ----------
% references:
% https://aclanthology.org/W05-0909.pdf

\clearpage
\thispagestyle{nlpstyle}
\section{METEOR}
\subsection{Metric for Evaluation of Translation with Explicit ORdering}

METEOR (Metric for Evaluation of Translation with Explicit ORdering) is a machine translation
evaluation metric introduced as an improvement over BLEU. Unlike BLEU, which relies heavily on
exact n-gram matches, METEOR aligns candidate translations with reference translations using stemming,
synonyms, and paraphrases. 

% equation
\begin{center}
    FORMULA GOES HERE
\end{center}

It then computes a harmonic mean of unigram precision and recall, with
recall weighted higher to better capture adequacy. Finally, it introduces a fragmentation penalty to
account for word order, rewarding translations that are both accurate and fluent.

\textbf{When to use METEOR?}

It is especially valuable when translation adequacy (capturing meaning) is as important as fluency.
Because it incorporates linguistic resources such as stemming and synonyms, METEOR is useful for
low-resource or morphologically rich languages, where exact matches are too strict.

\coloredboxes{
\item Closer to human judgment. METEOR was shown to correlate better with human evaluations compared
to BLEU.
\item Flexible matching. Accounts for stemming, synonyms, and paraphrases, making it more tolerant
to variation in expression.
\item Balances adequacy and fluency. By combining precision, recall, and word order penalties,
it reflects both meaning preservation and readability.
}
{
\item Computationally heavy. Requires linguistic resources (stemming, synonym databases),
making it slower than purely statistical metrics.
\item Language dependence. Its quality depends on the availability and completeness of stemming
rules and synonym dictionaries for the target language.
}

\clearpage

\thispagestyle{customstyle}

\orangebox{Did you know that...}
{METEOR was developed at Carnegie Mellon University in 2005 as a direct response to BLEU’s shortcomings.}


% ---------- Recall-Oriented Understudy for Gisting Evaluation ----------
% references:
% https://aclanthology.org/W04-1013.pdf
% https://aclanthology.org/E17-2007.pdf

\clearpage
\thispagestyle{nlpstyle}
\section{ROUGE}
\subsection{Recall-Oriented Understudy for Gisting Evaluation}

ROUGE is a family of metrics designed to evaluate automatic summarization and machine translation by
comparing the overlap between a system-generated text and one or more reference texts.
Unlike precision-oriented measures, ROUGE emphasizes recall, how much of the important content from
the reference is captured by the system output.

% equation
\begin{center}
    FORMULA GOES HERE
\end{center}

ROUGE scores range between 0 and 1, with higher values indicating higher similarity between the
system-generated text and the reference.

\textbf{When to use ROUGE?}

ROUGE is widely used in text summarization, where recalling important phrases is crucial, in machine
translation evaluation, and in other generation tasks such as captioning or dialogue, when
benchmarking against human-written references.

\coloredboxes{
\item Easy to compute and understand.
\item Many flexible variants available: ROUGE-N, ROUGE-L, and ROUGE-S capture different linguistic
overlaps (local n-grams, subsequences, or flexible structures).
}
{
\item ROUGE relies on lexical overlap and often misses semantic similarity.
\item Some studies have found that even human-written summaries can score poorly under ROUGE.
}

\clearpage

\thispagestyle{customstyle}

\orangebox{Did you know that...}
{ROUGE was introduced by Chin-Yew Lin in 2004 and quickly became a popular benchmark in text
summarization challenges. Despite its limitations, it is still one of the most reported
metrics in NLP.}

% ---------- Translation Error Rate ----------
% reference: https://aclanthology.org/2006.amta-papers.25.pdf
\clearpage
\thispagestyle{nlpstyle}
\section{TER}
\subsection{Translation Error Rate}

TER is a metric designed to evaluate machine translation quality by measuring the number of edits
required to change a system’s translation into a reference translation. The edits include insertions,
deletions, substitutions, and shifts of word sequences.

% equation
\begin{center}
    FORMULA GOES HERE
\end{center}

The score ranges from 0 to +infinity, where 0 indicates a perfect translation (no edits required).
Higher values correspond to translations needing more post-editing.

\textbf{When to use TER?}

Use TER when you want a metric that correlates closely with the amount of human editing effort
required to correct machine translations.

\coloredboxes{
\item Easy to compute and understand.
\item Includes word reordering (shifts), which captures errors that BLEU and related metrics
often miss.
}
{
\item Does not account for semantic equivalence when different but correct word choices are used.
\item TER tends to overestimate the actual translation error rate compared to human
post-editing effort.
}

\clearpage

\thispagestyle{customstyle}

\orangebox{Did you know that...}
{TER was introduced by Snover et al. (2006) to provide a more human-centered evaluation of
machine translation.}

% ---------- Exact Match ----------
% reference: https://nlp.stanford.edu/pubs/rajpurkar2016squad.pdf
\clearpage
\thispagestyle{nlpstyle}
\section{EM}
\subsection{Exact Match}

Exact Match measures the percentage of predictions that exactly match the ground-truth answers,
word-for-word and character-for-character. The metric returns 1 if the predicted output is an
exact match with the reference text, and 0 otherwise. The overall score is the average across all
samples.

% equation
\begin{center}
    FORMULA GOES HERE
\end{center}

EM is considered one of the strictest evaluation metrics since even a small deviation, such as an
extra space, different casing, or synonymous phrasing, leads to a score of 0.

\textbf{When to use EM?}

Use EM when exactness is critical. For instance, in extractive question answering, where the predicted
span must exactly match the gold answer. In structured outputs, such as JSON key-value generation,
where even a single mismatch makes the output invalid.

\coloredboxes{
\item Easy to calculate and explain. The prediction is either right or wrong.
\item Strict correctness guarantee. Particularly valuable when any deviation from the ground truth
is unacceptable (e.g., API calls, database queries).
}
{
\item Overly rigid. Does not account for semantically equivalent answers.
\item Not suitable for open-ended generation.
}

\clearpage

\orangebox{Did you know that...}
{Exact Match is one of the core metrics used in the Stanford Question Answering Dataset (SQuAD)
benchmark. However, because it is so strict, researchers often report both EM and F1 score.
EM for exact correctness, and F1 to give partial credit when a prediction overlaps significantly with
the ground truth.}