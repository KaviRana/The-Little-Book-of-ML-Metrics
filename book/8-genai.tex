\chapter{GenAI}


% ---------- Perplexity ----------
\clearpage
\thispagestyle{genaistyle}
\section{Perplexity}
\subsection{Perplexity}

% reference:
% https://aclanthology.org/2021.deelio-1.5.pdf
% https://kilthub.cmu.edu/articles/journal_contribution/Evaluation_Metrics_For_Language_Models/6605324/1?file=12095765
% https://huggingface.co/docs/transformers/perplexity
% https://brenocon.com/blog/2013/01/perplexity-as-branching-factor-as-shannon-diversity-index/

Perplexity is a widely used metric for evaluating language models. At its core, perplexity measures how well a probability distribution or model
predicts a sample. Mathematically, it is defined as the exponential of the average negative log-likelihood of the sequence. Intuitively, you can
think of it as the “average branching factor” — the number of equally likely choices the model considers at each step.

% equation
\begin{center}
    FORMULA GOES HERE
\end{center}

A lower perplexity indicates that the model assigns higher probabilities to the observed text, meaning it is less “surprised”.

\textbf{When to use Perplexity?}

Perplexity is most commonly used when evaluating language models during training or benchmarking. It helps track how well a model fits text data and
is especially useful for comparing models trained on the same dataset. However, perplexity is best suited for probabilistic next-token prediction
tasks and less reliable as a direct measure of end-user text quality.

\coloredboxes{
\item Simple and interpretable. Lower perplexity values generally mean better predictive performance.
\item Efficient for training evaluation. It provides a direct signal for model optimization without requiring human evaluation.
}
{
\item Models with low perplexity may still generate incoherent or unhelpful text.
\item Sensitive to tokenization and normalization choices.
\item Not well-defined for masked language models. Architectures like BERT predict missing tokens rather than generating sequences left-to-right,
making perplexity unsuitable for evaluating them.
}

\clearpage

\thispagestyle{customstyle}

\orangebox{Did you know that...}
{Perplexity is closely related to entropy in information theory. In fact, $Perplexity = 2^{H(P)}$
where \(H(P)\) is the entropy. This means that perplexity can be seen as the effective 
number of equally likely words the model is choosing from at each step!}

% ---------- BERTScore ----------
\clearpage
\thispagestyle{genaistyle}
\section{BERTScore}
\subsection{BERTScore}

% reference:
% https://arxiv.org/pdf/1904.09675
% https://wiki.math.uwaterloo.ca/statwiki/index.php?title=BERTScore:_Evaluating_Text_Generation_with_BERT

BERTScore is a metric for evaluating text generation that leverages contextual embeddings from pre-trained language models like BERT.
Instead of relying solely on surface-level n-gram overlap (as in BLEU or ROUGE), BERTScore computes similarity by aligning tokens from the
candidate and reference sentences in embedding space.

% equation
\begin{center}
    FORMULA GOES HERE
\end{center}

Mathematically, for each token in a candidate sentence, BERTScore finds its most similar token in the reference sentence (and vice versa)
using cosine similarity. Precision, recall, and F1 are then aggregated over all pairs, yielding a semantic-oriented score that correlates
strongly with human judgments.

\textbf{When to use the BERTScore?}

Use BERTScore when evaluating tasks where semantic similarity matters more than exact wording, such as machine translation, summarization, or
dialogue generation. It is especially useful when generated text can be phrased differently from references but still convey the same meaning.

\coloredboxes{
\item Context-aware. Uses deep contextual embeddings, capturing meaning beyond surface word matches.
\item Better correlation with humans. Empirical studies show BERTScore aligns more closely with human evaluation than BLEU or ROUGE.
}
{
\item Computationally heavy. Requires embedding extraction with large pre-trained models, making it slower than n-gram metrics.
\item Model dependence. Performance varies depending on which pre-trained model (e.g., BERT, RoBERTa, multilingual-BERT) is used.
\item Bias inheritance. Any biases in the underlying language model embeddings can influence the scores.
}

\clearpage

\thispagestyle{customstyle}

\orangebox{Did you know that...}
{The original BERTScore paper included a picture of Bert from Sesame Street paying homage to the model’s namesake and adding a playful touch to an
otherwise technical paper.}

% ---------- MAUVE ----------
\clearpage
\thispagestyle{genaistyle}
\section{MAUVE}
\subsection{MAUVE}

% reference:
% https://arxiv.org/pdf/2212.14578
% https://arxiv.org/pdf/2102.01454
% https://krishnap25.github.io/mauve/

MAUVE is a metric that measures how close the text written by a model is to the distribution of human text. Instead of checking for word overlap,
it looks at the shape of the two text distributions. It does this by taking samples from both human and model outputs, turning them into embeddings
using a large language model, and then comparing them. The comparison is done using a KL divergence in this embedding space. 

% equation
\begin{center}
    FORMULA GOES HERE
\end{center}

In simple terms, MAUVE asks: “If I look at a pile of human texts and a pile of model texts, do they look like they came from the same source?”
A higher MAUVE score means the model text is statistically closer to human text.

\textbf{When to use the MAUVE?}

Use MAUVE when you want to evaluate open-ended generation tasks like stories, dialogues, or summaries. It’s especially helpful when you care
about both fluency and diversity.

\coloredboxes{
\item Balances quality and diversity. Unlike BLEU or ROUGE, MAUVE doesn’t just check for overlap. It rewards outputs that are fluent and varied.
\item MAUVE has been shown to correlate strongly with how humans rate generated text.
}
{
\item Works best with thousands of examples (the original paper used 5,000). With fewer, results may be unstable.
\item MAUVE only measures similarity as seen by the embedding model. If the style, topics, or length of texts differ, good text can still get
a low score.
\item Heavy to run. By default, it uses GPT-2 large embeddings. Smaller models can be used, but results may differ.
}

\clearpage

\thispagestyle{customstyle}

\orangebox{Did you know that...}
{MAUVE was introduced in 2021 and quickly became popular because it correlated better with human ratings than older metrics like perplexity.
It even won a Outstanding Paper Award at NeurIPS 2021.}


% ---------- Inception Score ---------
\clearpage
\thispagestyle{genaistyle}
\section{IS}
\subsection{Inception Score}

The Inception Score (IS) is a popular metric for evaluating generative image models, especially GANs. It measures both the quality and the
diversity of generated images by using a pre-trained Inception v3 network.

% equation
\begin{center}
    FORMULA GOES HERE
\end{center}

The idea is simple, a good generative model should produce images that are easily classifiable into distinct categories (high confidence
predictions) and cover a wide range of categories overall (diverse outputs). IS combines these two aspects into a single score,
where higher is better.

\textbf{When to use IS?}

Use IS when you want a reference-free evaluation metric for image generative models.
It’s often used to compare GAN architectures during research and prototyping.


\coloredboxes{
\item IS evaluates generated images directly, without requiring access to ground-truth samples.
\item Encourages models to generate sharp, realistic, and varied images.
}
{
\item A model can achieve a high score while still producing limited or repetitive variations within each class.
\item Since it relies on the Inception v3 network trained on ImageNet, it may not be meaningful for domains far from natural
images (e.g., medical scans, satellite imagery).
}

\clearpage

\thispagestyle{customstyle}

\orangebox{Did you know that...}
{The Inception Score was introduced in 2016 by Tim Salimans, Ian Goodfellow and colleagues at OpenAI in their paper Improved Techniques
for Training GANs. It quickly became the standard benchmark for evaluating GANs, since it offered one of the first automated ways to score
generated images without human annotators.}

% ---------- Fréchet Inception Distance ---------
\clearpage
\thispagestyle{genaistyle}
\section{FID}
\subsection{Fréchet Inception Distance}

% reference:
% https://papers.nips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf
% https://arxiv.org/pdf/2401.09603

The Fréchet Inception Distance (FID) is one of the most widely used metrics to evaluate the quality of images produced by generative models,
such as GANs. Instead of comparing pixels directly, FID compares the distributions of generated and real images in a feature space extracted
from a pretrained Inception-v3 network.

% equation
\begin{center}
    FORMULA GOES HERE
\end{center}

Mathematically, it models these feature distributions as multivariate Gaussians and computes the Fréchet distance Wasserstein-2 distanc between them.
A smaller FID score indicates that generated images are closer to the real ones in terms of both quality and diversity.

\textbf{When to use FID?}

Use FID when you want to evaluate image generation quality and diversity in a way that aligns better with human perception than pixel-level metrics.
It is especially useful in comparing different generative models or monitoring improvements during training.

\coloredboxes{
\item By working in the feature space of a neural network trained on ImageNet, FID captures semantic similarity rather than raw pixel similarity.
\item Diversity-aware. Unlike Inception Score, FID penalizes mode collapse (when the model generates limited varieties of images).
}
{
\item FID is statistically biased. Its expected value on finite samples does not equal the true value, which can lead to misleading
comparisons when datasets are small.
\item Not suited for all setups. FID measures the Wasserstein distance to a fixed ground-truth distribution, so it is inadequate for
evaluating models in domain adaptation or zero-shot generation, where the target distribution is unclear.
\item FID assumes Inception embeddings follow a multivariate normal distribution.
}

\clearpage

\thispagestyle{customstyle}

\orangebox{Did you know that...}
{The Fréchet Inception Distance gets its name from Maurice Fréchet, a French mathematician who introduced the concept of metric spaces in 1906.
More than a century later, his work is now central to evaluating the realism of AI-generated images!}

\textbf{FID vs IS}

The main difference with the Inception Score (IS) is that while IS only measures the quality and diversity of generated images, FID also
compares them directly to real images, making it more sensitive to how closely the generated distribution matches reality.


% ---------- Learned Perceptual Image Patch Similarity ---------
\clearpage
\thispagestyle{genaistyle}
\section{LPIPS}
\subsection{Learned Perceptual Image Patch Similarity}

% references: https://arxiv.org/pdf/1801.03924

LPIPS calculates the perceptual similarity between two images, aiming to reflect how similar they appear to human observers.
Instead of comparing pixel values directly(like PSNR or SSIM), LPIPS uses the internal activations of deep neural networks trained for
visual recognition. The idea is that these network features capture aspects of visual perception better than raw pixels.

% equation
\begin{center}
    FORMULA GOES HERE
\end{center}

LPIPS calculates perceptual similarity between two images using deep network features. Features are extracted from different layers of a
pretrained neural network, normalized, and then compared. These distances are averaged and weighted so they align with human perceptual judgments.
A lower LPIPS score means images look more alike and higher scores mean they appear more different.

\textbf{When to use LPIPS?}

Use LPIPS when you want to evaluate how similar generated or reconstructed images feel to humans, rather than how close they are pixel by pixel.
It is widely used in image generation, style transfer, and super-resolution tasks where perceptual quality is more important than exact fidelity.

\coloredboxes{
\item LPIPS correlates well with human judgments of similarity, often outperforming traditional metrics like SSIM and FSIM.
\item Handles complex distortions and perceptual changes that pixel-based metrics miss.
}
{
\item Since LPIPS relies on deep network features, it can be tricked by adversarial perturbations.
\item More expensive to compute than simpler metrics like PSNR or SSIM.
}

\clearpage

\thispagestyle{customstyle}

\orangebox{Did you know that...}
{LPIPS was introduced in 2018 by Richard Zhang and colleagues at UC Berkeley, OpenAI and Adobe. They showed that off-the-shelf deep features—even
from different architectures—already align surprisingly well with human perception. Later, they refined the metric by learning weights from
large-scale human preference studies.}


% ---------- CLIPScore ---------
\clearpage
\thispagestyle{genaistyle}
\section{CLIP Score}
\subsection{CLIP Score}

% reference:
% https://arxiv.org/pdf/2104.08718
% https://lightning.ai/docs/torchmetrics/stable/multimodal/clip_score.html

The CLIP Score is a reference-free metric used to evaluate how well a generated caption matches the
actual content of an image. It relies on the CLIP model, which embeds both text and images into the same semantic space. To compute it,
the image and the candidate caption are passed through CLIP’s feature extractors, and their cosine similarity is measured. The score is then
rescaled, so higher values indicate stronger alignment between text and image.

% equation
\begin{center}
    % CLIP-S(c,v)=w⋅max(cos(c,v),0)
    FORMULA GOES HERE
\end{center}

A higher CLIP Score indicates better alignment between text and image. The score ranges from 0 (no alignment) up to w (the scaling weight,
commonly set to 2.5), with larger values reflecting stronger semantic similarity.

\textbf{When to use CLIP Score?}

Use CLIP Score when you need to assess the quality of text-to-image or image-to-text generation without requiring human-written reference captions.
It is particularly useful for evaluating generative models in tasks like image captioning, text-to-image synthesis, or retrieval,
where semantic alignment matters more than exact word overlap.

\coloredboxes{
\item Does not require ground-truth captions.
\item Correlates with human judgment. Has been shown to align well with how humans assess caption-image relevance.
}
{
\item Model-dependent. The quality of the score depends heavily on the CLIP model and its training data; biases in CLIP can affect results.
\item Insensitive to linguistic nuances. Two captions with very different readability or fluency may still achieve similar scores
if they mention the right objects.
\item Prompt sensitivity. Small wording changes (e.g., starting captions with “A photo of...”) can influence the score.
}

\clearpage

\thispagestyle{customstyle}

\orangebox{Did you know that...}
{The CLIP Score was introduced in 2022 by researchers at the Allen Institute for AI and The University of Washington and has quickly become a
standard way to evaluate multimodal AI, thanks to the popularity of CLIP, which was originally developed by OpenAI.}


% ---------- Davidsonian Scene Graph ---------
\clearpage
\thispagestyle{genaistyle}
\section{DSG}
\subsection{Davidsonian Scene Graph}

The Davidsonian Scene Graph (DSG) measures how well a text-to-image model captures the meaning of a prompt by checking if the image has the right 
objects, attributes, and relations—not just if it looks good.

% no formula for this metric
It builds on the idea of scene graphs—structured representations of an image in terms of “who does what to whom.” For example, the sentence
“a cat sitting on a chair” can be represented as cat — sitting on — chair.

DSG checks this alignment through two steps. First, a Question Generation module takes the prompt and breaks it into small yes/no questions
that cover its meaning. It asks things like “Is there a cat?”, “Is it sitting?”, “Is it on a chair?”. These questions are arranged in a graph,
so if one answer is “no” (say there is no cat), then the follow-up questions about that cat are skipped.

Then, a Visual Question Answering module asks the same questions to the image. If the answers from the image match the answers from the text,
the scene is captured correctly.

The final DSG score is simply how well the image answered the full set of questions derived from the prompt. A higher score means the picture
respects the objects, attributes, and relations described in the text.


\textbf{When to use DSG?}

Use DSG when you want to evaluate whether an image generation model respects the semantic structure of a text prompt, not just
superficial resemblance. It is especially useful for benchmarking text-to-image systems on prompts that involve multiple entities and
relationships, such as “a person holding a red umbrella next to a dog.”

\coloredboxes{
\item Semantics-aware. Goes beyond pixels and aesthetics by checking whether objects and their relations are correct.
\item Captures whether multiple parts of a prompt are faithfully represented, not just the most prominent one.
}
{
\item Some subtle visual details (e.g., style, texture, artistic quality) are not captured.
\item Building and comparing scene graphs is more complex than simple embedding-based metrics like CLIPScore.
}


\clearpage

\thispagestyle{customstyle}

\orangebox{Did you know that...}
{The “Davidsonian” in Davidsonian Scene Graph comes from the American philosopher Donald Davidson, who argued that the meaning of a sentence can
be broken down into atomic events and their relations. DSG borrows this idea, turning a text prompt into small, testable propositions like
“there is a cat” or “the cat sits on the chair.”}